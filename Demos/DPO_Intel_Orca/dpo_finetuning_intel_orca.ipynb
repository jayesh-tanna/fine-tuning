{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DPO Fine-Tuning with Intel Orca Dataset on Azure AI\n",
        "\n",
        "This notebook demonstrates how to fine-tune language models using **Direct Preference Optimization (DPO)** with the Intel Orca DPO Pairs dataset.\n",
        "\n",
        "## What You'll Learn\n",
        "1. Understand DPO fine-tuning\n",
        "2. Prepare and format DPO training data  \n",
        "3. Upload datasets to Azure AI\n",
        "4. Create and monitor a DPO fine-tuning job\n",
        "5. Evaluate your fine-tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "Install all required packages from requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: azure-ai-projects>=2.0.0b1 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from -r requirements.txt (line 2)) (2.0.0b2)\n",
            "Requirement already satisfied: azure-identity in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from -r requirements.txt (line 3)) (1.25.1)\n",
            "Collecting openai (from -r requirements.txt (line 4))\n",
            "  Downloading openai-2.9.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: azure-mgmt-cognitiveservices>=13.0.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from -r requirements.txt (line 7)) (14.1.0)\n",
            "Requirement already satisfied: python-dotenv>=1.0.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from -r requirements.txt (line 10)) (1.2.1)\n",
            "Requirement already satisfied: isodate>=0.6.1 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (0.7.2)\n",
            "Requirement already satisfied: azure-core>=1.35.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (1.36.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: azure-storage-blob>=12.15.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (12.27.1)\n",
            "Requirement already satisfied: cryptography>=2.5 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-identity->-r requirements.txt (line 3)) (46.0.3)\n",
            "Requirement already satisfied: msal>=1.30.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-identity->-r requirements.txt (line 3)) (1.34.0)\n",
            "Requirement already satisfied: msal-extensions>=1.2.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-identity->-r requirements.txt (line 3)) (1.3.1)\n",
            "Collecting anyio<5,>=3.5.0 (from openai->-r requirements.txt (line 4))\n",
            "  Downloading anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai->-r requirements.txt (line 4))\n",
            "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from openai->-r requirements.txt (line 4))\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.10.0 (from openai->-r requirements.txt (line 4))\n",
            "  Downloading jiter-0.12.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
            "Collecting pydantic<3,>=1.9.0 (from openai->-r requirements.txt (line 4))\n",
            "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
            "     ---------------------------------------- 0.0/90.6 kB ? eta -:--:--\n",
            "     ------------------ --------------------- 41.0/90.6 kB 1.9 MB/s eta 0:00:01\n",
            "     ------------------------------ ------- 71.7/90.6 kB 787.7 kB/s eta 0:00:01\n",
            "     -------------------------------------- 90.6/90.6 kB 730.4 kB/s eta 0:00:00\n",
            "Collecting sniffio (from openai->-r requirements.txt (line 4))\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting tqdm>4 (from openai->-r requirements.txt (line 4))\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: msrest>=0.7.1 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-mgmt-cognitiveservices>=13.0.0->-r requirements.txt (line 7)) (0.7.1)\n",
            "Requirement already satisfied: azure-mgmt-core>=1.6.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-mgmt-cognitiveservices>=13.0.0->-r requirements.txt (line 7)) (1.6.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 4)) (3.11)\n",
            "Requirement already satisfied: requests>=2.21.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (2.32.5)\n",
            "Requirement already satisfied: cffi>=2.0.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from cryptography>=2.5->azure-identity->-r requirements.txt (line 3)) (2.0.0)\n",
            "Requirement already satisfied: certifi in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 4)) (2025.11.12)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 4))\n",
            "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 4))\n",
            "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity->-r requirements.txt (line 3)) (2.10.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from msrest>=0.7.1->azure-mgmt-cognitiveservices>=13.0.0->-r requirements.txt (line 7)) (2.0.0)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 4))\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 4))\n",
            "  Downloading pydantic_core-2.41.5-cp311-cp311-win_amd64.whl.metadata (7.4 kB)\n",
            "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 4))\n",
            "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: colorama in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from tqdm>4->openai->-r requirements.txt (line 4)) (0.4.6)\n",
            "Requirement already satisfied: pycparser in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from cffi>=2.0.0->cryptography>=2.5->azure-identity->-r requirements.txt (line 3)) (2.23)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.35.0->azure-ai-projects>=2.0.0b1->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\work\\amlrepos\\fine-tuning\\env\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.7.1->azure-mgmt-cognitiveservices>=13.0.0->-r requirements.txt (line 7)) (3.3.1)\n",
            "Downloading openai-2.9.0-py3-none-any.whl (1.0 MB)\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 0.1/1.0 MB 4.2 MB/s eta 0:00:01\n",
            "   ---------- ----------------------------- 0.3/1.0 MB 2.8 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 0.5/1.0 MB 3.5 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 0.9/1.0 MB 4.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.0/1.0 MB 4.7 MB/s eta 0:00:00\n",
            "Downloading anyio-4.12.0-py3-none-any.whl (113 kB)\n",
            "   ---------------------------------------- 0.0/113.4 kB ? eta -:--:--\n",
            "   ---------------------------------------- 113.4/113.4 kB 6.4 MB/s eta 0:00:00\n",
            "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Downloading jiter-0.12.0-cp311-cp311-win_amd64.whl (204 kB)\n",
            "   ---------------------------------------- 0.0/204.9 kB ? eta -:--:--\n",
            "   --------------------------------------- 204.9/204.9 kB 12.2 MB/s eta 0:00:00\n",
            "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
            "   ---------------------------------------- 0.0/463.6 kB ? eta -:--:--\n",
            "   --------------------------------------- 463.6/463.6 kB 30.2 MB/s eta 0:00:00\n",
            "Downloading pydantic_core-2.41.5-cp311-cp311-win_amd64.whl (2.0 MB)\n",
            "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
            "   ----------------------- ---------------- 1.2/2.0 MB 37.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------  2.0/2.0 MB 21.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.0/2.0 MB 18.4 MB/s eta 0:00:00\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
            "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: typing-inspection, tqdm, sniffio, pydantic-core, jiter, h11, distro, anyio, annotated-types, pydantic, httpcore, httpx, openai\n",
            "Successfully installed annotated-types-0.7.0 anyio-4.12.0 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jiter-0.12.0 openai-2.9.0 pydantic-2.12.5 pydantic-core-2.41.5 sniffio-1.3.1 tqdm-4.67.1 typing-inspection-0.4.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " All libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "\n",
        "print(\" All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configure Azure Environment\n",
        "Set your Azure AI Project endpoint and model name. We're using **gpt-4o-mini** in this example, but you can use other supported GPT models. Create a `.env` file with: \n",
        "\n",
        "```\n",
        "AZURE_AI_PROJECT_ENDPOINT=<your-endpoint> \n",
        "MODEL_NAME=gpt-4o-mini\n",
        "AZURE_SUBSCRIPTION_ID=<your-subscription-id>\n",
        "AZURE_RESOURCE_GROUP=<your-resource-group>\n",
        "AZURE_AOAI_ACCOUNT=<your-foundry-account-name>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Endpoint: https://foundrysdk-eastus2-foundry-resou.services.ai.azure.com/api/projects/foundrysdk-eastus2-project\n",
            " Model: gpt-4o-mini\n"
          ]
        }
      ],
      "source": [
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "endpoint = os.environ.get(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
        "model_name = os.environ.get(\"MODEL_NAME\")\n",
        "\n",
        "# Define dataset file paths\n",
        "training_file_path = \"training.jsonl\"\n",
        "validation_file_path = \"validation.jsonl\"\n",
        "\n",
        "print(f\" Endpoint: {endpoint}\")\n",
        "print(f\" Model: {model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Connect to Azure AI Project\n",
        "\n",
        "Connect to Azure AI Project using Azure credential authentication. This initializes the project client and OpenAI client needed for fine-tuning workflows. Ensure you have the **Azure AI User** role assigned to your account for the Azure AI Project resource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Connected to Azure AI Project\n"
          ]
        }
      ],
      "source": [
        "credential = DefaultAzureCredential()\n",
        "project_client = AIProjectClient(endpoint=endpoint, credential=credential)\n",
        "openai_client = project_client.get_openai_client()\n",
        "\n",
        "print(\" Connected to Azure AI Project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Upload Training Files\n",
        "\n",
        "Upload the training and validation JSONL files to Azure AI. Each file is assigned a unique ID that will be referenced when creating the fine-tuning job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading training file...\n",
            " Training file ID: file-b137633d81d54f0dbf19103c0a76214b\n",
            "\n",
            "Uploading validation file...\n",
            " Validation file ID: file-577ad4366204477486a29751fbdfb93c\n"
          ]
        }
      ],
      "source": [
        "print(\"Uploading training file...\")\n",
        "with open(training_file_path, \"rb\") as f:\n",
        "    train_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
        "print(f\" Training file ID: {train_file.id}\")\n",
        "\n",
        "print(\"\\nUploading validation file...\")\n",
        "with open(validation_file_path, \"rb\") as f:\n",
        "    validation_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n",
        "print(f\" Validation file ID: {validation_file.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for files to be processed...\n",
            " Files ready!\n"
          ]
        }
      ],
      "source": [
        "print(\"Waiting for files to be processed...\")\n",
        "openai_client.files.wait_for_processing(train_file.id)\n",
        "openai_client.files.wait_for_processing(validation_file.id)\n",
        "print(\" Files ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create DPO Fine-Tuning Job\n",
        "Create a DPO fine-tuning job with your uploaded datasets. Configure the following hyperparameters to control the training process:\n",
        "\n",
        "1. n_epochs (3): Number of complete passes through the training dataset. More epochs can improve performance but may lead to overfitting. Typical range: 1-10.\n",
        "2. batch_size (1): Number of training examples processed together in each iteration. Smaller batches (1-2) are common for DPO to maintain training stability.\n",
        "3. learning_rate_multiplier (1.0): Scales the default learning rate. Values < 1.0 make training more conservative, while values > 1.0 speed up learning but may cause instability. Typical range: 0.1-2.0.\n",
        "Adjust these values based on your dataset size and desired model behavior. \n",
        "\n",
        "Start with these defaults and experiment if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Job ID: ftjob-6aa173fb0c9d4c44b2fb09d9389db5e7\n",
            "Status: pending\n"
          ]
        }
      ],
      "source": [
        "fine_tuning_job = openai_client.fine_tuning.jobs.create(\n",
        "    training_file=train_file.id,\n",
        "    validation_file=validation_file.id,\n",
        "    model=model_name,\n",
        "    method={\n",
        "        \"type\": \"dpo\",\n",
        "        \"dpo\": {\n",
        "            \"hyperparameters\": {\n",
        "                \"n_epochs\": 3,\n",
        "                \"batch_size\": 1,\n",
        "                \"learning_rate_multiplier\": 1.0\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    extra_body={\"trainingType\": \"Standard\"}\n",
        ")\n",
        "\n",
        "print(f\" Job ID: {fine_tuning_job.id}\")\n",
        "print(f\"Status: {fine_tuning_job.status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Monitor Training Progress\n",
        "Check the status of your fine-tuning job and track progress. You can view the current status, and recent training events. Training duration varies based on dataset size, model, and hyperparameters - typically ranging from minutes to several hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: pending\n"
          ]
        }
      ],
      "source": [
        "job_status = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
        "print(f\"Status: {job_status.status}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jobs ahead in queue: 6\n",
            "Job enqueued. Waiting for jobs ahead to complete.\n"
          ]
        }
      ],
      "source": [
        "# View recent events\n",
        "events = list(openai_client.fine_tuning.jobs.list_events(fine_tuning_job.id, limit=10))\n",
        "for event in events:\n",
        "    print(event.message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Retrieve Fine-Tuned Model\n",
        "After the fine-tuning job succeeded, retrieve the fine-tuned model ID. This ID is required to make inference calls with your customized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: pending\n"
          ]
        }
      ],
      "source": [
        "completed_job = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
        "\n",
        "if completed_job.status == \"succeeded\":\n",
        "    fine_tuned_model_id = completed_job.fine_tuned_model\n",
        "    print(f\" Fine-tuned Model ID: {fine_tuned_model_id}\")\n",
        "else:\n",
        "    print(f\"Status: {completed_job.status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Deploy the fine-tuned Model\n",
        "\n",
        "Deploy the fine-tuned model to Azure OpenAI as a deployment endpoint. This step is required before making inference calls. The deployment uses GlobalStandard SKU with 50 TPM capacity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
        "from azure.mgmt.cognitiveservices.models import Deployment, DeploymentProperties, DeploymentModel, Sku\n",
        "import time\n",
        "\n",
        "subscription_id = os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
        "resource_group = os.environ.get(\"AZURE_RESOURCE_GROUP\")\n",
        "account_name = os.environ.get(\"AZURE_AOAI_ACCOUNT\")\n",
        "\n",
        "deployment_name = \"gpt-4o-mini-dpo-finetuned\"\n",
        "\n",
        "with CognitiveServicesManagementClient(credential=credential, subscription_id=subscription_id) as cogsvc_client:\n",
        "    deployment_model = DeploymentModel(format=\"OpenAI\", name=fine_tuned_model_id, version=\"1\")\n",
        "    deployment_properties = DeploymentProperties(model=deployment_model)\n",
        "    deployment_sku = Sku(name=\"GlobalStandard\", capacity=50)\n",
        "    deployment_config = Deployment(properties=deployment_properties, sku=deployment_sku)\n",
        "    \n",
        "    print(f\"Deploying fine-tuned model: {fine_tuned_model_id}\")\n",
        "    deployment = cogsvc_client.deployments.begin_create_or_update(\n",
        "        resource_group_name=resource_group,\n",
        "        account_name=account_name,\n",
        "        deployment_name=deployment_name,\n",
        "        deployment=deployment_config,\n",
        "    )\n",
        "    \n",
        "    while deployment.status() not in [\"Succeeded\", \"Failed\"]:\n",
        "        time.sleep(30)\n",
        "        print(f\"Deployment status: {deployment.status()}\")\n",
        "\n",
        "print(f\" Model deployment completed: {deployment_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Test Your Fine-Tuned Model\n",
        "\n",
        "Validate your fine-tuned model by running test inferences. This helps you assess whether the DPO training successfully aligned the model with your preferred response patterns from the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Testing fine-tuned model via deployment: {deployment_name}\")\n",
        "\n",
        "response = openai_client.responses.create(\n",
        "    model=deployment_name,\n",
        "    input=[{\"role\": \"user\", \"content\": \"Explain machine learning in simple terms.\"}]\n",
        ")\n",
        "\n",
        "print(f\"Model response: {response.output_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Next Steps\n",
        "\n",
        "Congratulations! You've successfully fine-tuned a model with DPO.\n",
        "\n",
        "### What's Next?\n",
        "- Deploy your model to production\n",
        "- Evaluate on more test cases\n",
        "- Experiment with hyperparameters\n",
        "- Try different datasets"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
